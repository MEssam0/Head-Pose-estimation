{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "USe Models trained on optimized data as its performance is very good videos as change in image i=and face width and height"
      ],
      "metadata": {
        "id": "Zz2AFGf6qdL3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "URPv6rg1nfdj"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install mediapipe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "LGLwcnNll3Mf"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import pickle\n",
        "from math import cos, sin, atan2\n",
        "import mediapipe\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab.patches import cv2_imshow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "dcvssfxvmqW2"
      },
      "outputs": [],
      "source": [
        "def draw_axis(img, pitch,yaw,roll, tdx=None, tdy=None, size = 100):\n",
        "    # print(pitch,yaw,roll)\n",
        "    # cv2_imshow(img)\n",
        "    yaw = -yaw\n",
        "    if tdx != None and tdy != None:\n",
        "        tdx = tdx\n",
        "        tdy = tdy\n",
        "    else:\n",
        "        height, width = img.shape[:2]\n",
        "        tdx = width / 2\n",
        "        tdy = height / 2\n",
        "\n",
        "    # X-Axis pointing to right. drawn in red\n",
        "    x1 = size * (cos(yaw) * cos(roll)) + tdx\n",
        "    y1 = size * (cos(pitch) * sin(roll) + cos(roll) * sin(pitch) * sin(yaw)) + tdy\n",
        "\n",
        "    # Y-Axis | drawn in green\n",
        "    #        v\n",
        "    x2 = size * (-cos(yaw) * sin(roll)) + tdx\n",
        "    y2 = size * (cos(pitch) * cos(roll) - sin(pitch) * sin(yaw) * sin(roll)) + tdy\n",
        "\n",
        "    # Z-Axis (out of the screen) drawn in blue\n",
        "    x3 = size * (sin(yaw)) + tdx\n",
        "    y3 = size * (-cos(yaw) * sin(pitch)) + tdy\n",
        "\n",
        "    cv2.line(img, (int(tdx), int(tdy)), (int(x1),int(y1)),(0,0,255),3)\n",
        "    cv2.line(img, (int(tdx), int(tdy)), (int(x2),int(y2)),(0,255,0),3)\n",
        "    cv2.line(img, (int(tdx), int(tdy)), (int(x3),int(y3)),(255,0,0),2)\n",
        "\n",
        "    return img"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract face features and return data frame containing these features and nose points to draw data from\n",
        "def extract_face_landmark(face_landmark):\n",
        "    data = dict()\n",
        "    for idx, lm in enumerate(face_landmark):\n",
        "        # features number 10 and 152 represents the most botton and top features in the face\n",
        "        dist = np.sqrt((face_landmark[152].x-face_landmark[10].x)**2 + (face_landmark[152].y-face_landmark[10].y)**2)\n",
        "\n",
        "        # feature number 1 represents nose point\n",
        "        dx = (lm.x - face_landmark[1].x) / dist\n",
        "        dy = (lm.y - face_landmark[1].y) / dist\n",
        "        data.setdefault(str(idx)+'_x',[])\n",
        "        data[str(idx)+'_x'].append(dx)\n",
        "        data.setdefault(str(idx)+'_y',[])\n",
        "        data[str(idx)+'_y'].append(dy)\n",
        "    return pd.DataFrame(data)"
      ],
      "metadata": {
        "id": "--HAbgSGbbXX"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def nose_point(face_landmark, img_h, img_w):\n",
        "    # feature number 1 represents nose point\n",
        "    nose_2d = (face_landmark[1].x * img_w, face_landmark[1].y * img_h)\n",
        "    return int(nose_2d[0]), int(nose_2d[1])"
      ],
      "metadata": {
        "id": "_E3OVWSne_lq"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(data,model_path):\n",
        "    model = pickle.load(open(model_path, 'rb'))\n",
        "    y_pred = model.predict(pd.DataFrame(data))\n",
        "\n",
        "    return  y_pred[0][0], y_pred[0][1], y_pred[0][2]"
      ],
      "metadata": {
        "id": "BvEAnoLGd7vk"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "m-9RmLB-lJrq"
      },
      "outputs": [],
      "source": [
        "cap = cv2.VideoCapture('/content/Pose_estimate video.mp4')\n",
        "i = 0\n",
        "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "\n",
        "\n",
        "fourcc = cv2.VideoWriter_fourcc('M','J','P','G')   \n",
        "out = cv2.VideoWriter('/content/draw_pose_final.avi', fourcc, fps, (frame_width, frame_height))\n",
        "faceModule = mediapipe.solutions.face_mesh\n",
        "with faceModule.FaceMesh(static_image_mode=True) as faces:\n",
        "  while cap.isOpened():\n",
        "      ret, image = cap.read()\n",
        "\n",
        "      if not ret:\n",
        "        break\n",
        "      # Flip the image horizontally for a later selfie-view display\n",
        "      # Also convert the color space from BGR to RGB\n",
        "      image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
        "      # To improve performance\n",
        "      image.flags.writeable = False\n",
        "      # Get the result\n",
        "      results = faces.process(image)\n",
        "      # To improve performance\n",
        "      image.flags.writeable = True\n",
        "      # Convert the color space from RGB to BGR\n",
        "      image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
        "      # for landmark in face.landmark:\n",
        "      img_h, img_w, img_c = image.shape\n",
        "      face_3d = []\n",
        "      face_2d = []\n",
        "      if results.multi_face_landmarks != None:\n",
        "          for face in results.multi_face_landmarks:\n",
        "              # extract face features\n",
        "              data_df = extract_face_landmark(face.landmark)\n",
        "              # extract x and y coordinates of nose point\n",
        "              nose_x, nose_y = nose_point(face.landmark, img_h, img_w)\n",
        "              # predict pitch yaw roll of the face from x y features\n",
        "              pitch, yaw, roll = predict(data_df,'/content/Model.sav')\n",
        "              # draw pitch yaw roll on the face\n",
        "              image = draw_axis(image,pitch,yaw,roll,nose_x,nose_y)\n",
        "          \n",
        "      out.write(image)\n",
        "\n",
        "      if cv2.waitKey(20) == ord('q'):\n",
        "          break\n",
        "\n",
        "      i+=1\n",
        "\n",
        "  cv2.destroyAllWindows()\n",
        "  cap.release()\n",
        "  # print('done')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "381hQRBcuxJO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}